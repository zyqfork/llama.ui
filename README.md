# ğŸ¦™ llama.ui - Minimal Interface for Local AI Companion âœ¨

**Tired of complex AI setups?** ğŸ˜© `llama.ui` is an open-source desktop application that provides a beautiful âœ¨, user-friendly interface for interacting with large language models (LLMs) powered by `llama.cpp`. Designed for simplicity and privacy ğŸ”’, this project lets you chat with powerful quantized models on your local machine - no cloud required! ğŸš«â˜ï¸

## âš¡ TL;DR

This repository is a fork of [llama.cpp](https://github.com/ggml-org/llama.cpp) WebUI with:

- Fresh new styles ğŸ¨
- Extra functionality âš™ï¸
- Smoother experience âœ¨

![welcome-screen](public/screenshots/desktop.png)

## ğŸŒŸ Key Features

1. **Multi-Provider Support**: Works with llama.cpp, LM Studio, Ollama, vLLM, OpenAI,.. and many more!

2. **Conversation Management**:
   - IndexedDB storage for conversations
   - Branching conversation support (edit messages while preserving history)
   - Import/export functionality

3. **Rich UI Components**:
   - Markdown rendering with syntax highlighting
   - LaTeX math support
   - File attachments (text, images, PDFs)
   - Theme customization with DaisyUI themes
   - Responsive design for mobile and desktop

4. **Advanced Features**:
   - PWA support with offline capabilities
   - Streaming responses with Server-Sent Events
   - Customizable generation parameters
   - Performance metrics display

5. **Privacy Focused**: All data is stored locally in your browser - no cloud required!

6. **Localized Interface**: Most popular language packs are included in the app, and you can choose the language at any time.

## ğŸš€ Getting Started in 60 Seconds!

### ğŸ’» Standalone Mode (Zero Installation)

1. âœ¨ Open our [hosted UI instance](https://llama-ui.js.org/)
2. âš™ï¸ Click the gear icon â†’ General settings
3. ğŸŒ Set "Base URL" to your local llama.cpp server (e.g. `http://localhost:8080`)
4. ğŸ‰ Start chatting with your AI!

<details><summary><b>ğŸ”§ Need HTTPS magic for your local instance? Try this mitmproxy hack!</b></summary>
<p>

**Uh-oh!** Browsers block HTTP requests from HTTPS sites ğŸ˜¤. Since `llama.cpp` uses HTTP, we need a bridge ğŸŒ‰. Enter [mitmproxy](https://www.mitmproxy.org/) - our traffic wizard! ğŸ§™â€â™‚ï¸

**Local setup:**

```bash
mitmdump -p 8443 --mode reverse:http://localhost:8080/
```

**Docker quickstart:**

```bash
docker run -it -p 8443:8443 mitmproxy/mitmproxy mitmdump -p 8443 --mode reverse:http://localhost:8080/
```

**Pro-tip with Docker Compose:**

```yml
services:
  mitmproxy:
    container_name: mitmproxy
    image: mitmproxy/mitmproxy:latest
    ports:
      - '8443:8443' # ğŸ” Port magic happening here!
    command: mitmdump -p 8443 --mode reverse:http://localhost:8080/
    # ... (other config)
```

> âš ï¸ **Certificate Tango Time!**
>
> 1. Visit http://localhost:8443
> 2. Click "Trust this certificate" ğŸ¤
> 3. Restart ğŸ¦™ llama.ui page ğŸ”„
> 4. Profit! ğŸ’¸

**VoilÃ !** You've hacked the HTTPS barrier! ğŸ©âœ¨

</p>
</details>

### ğŸ–¥ï¸ Full Local Installation (Power User Edition)

1. ğŸ“¦ Grab the latest release from our [releases page](https://github.com/olegshulyakov/llama.ui/releases)
2. ğŸ—œï¸ Unpack the archive (feel that excitement! ğŸ¤©)
3. âš¡ Fire up your llama.cpp server:

**Linux/MacOS:**

```bash
./server --host 0.0.0.0 \
         --port 8080 \
         --path "/path/to/llama.ui" \
         -m models/llama-2-7b.Q4_0.gguf \
         --ctx-size 4096
```

**Windows:**

```bat
llama-server ^
             --host 0.0.0.0 ^
             --port 8080 ^
             --path "C:\path\to\llama.ui" ^
             -m models\mistral-7b.Q4_K_M.gguf ^
             --ctx-size 4096
```

4. ğŸŒ Visit http://localhost:8080 and meet your new AI buddy! ğŸ¤–â¤ï¸

## ğŸŒŸ Join Our Awesome Community!

**We're building something special together!** ğŸš€

- ğŸ¯ **PRs are welcome!** (Seriously, we high-five every contribution! âœ‹)
- ğŸ› **Bug squashing?** Yes please! ğŸ§¯
- ğŸ“š **Documentation heroes** needed! ğŸ¦¸
- âœ¨ **Make magic** with your commits! (Follow [Conventional Commits](https://www.conventionalcommits.org))

## ğŸ› ï¸ Developer Wonderland

**Prerequisites:**

- ğŸ’» macOS/Windows/Linux
- â¬¢ [Node.js](https://nodejs.org/) >= 22
- ğŸ¦™ Local [llama.cpp server](https://github.com/ggml-org/llama.cpp/tree/master/tools/server) humming along

**Build the future:**

```bash
npm ci       # ğŸ“¦ Grab dependencies
npm run build  # ğŸ”¨ Craft the magic
npm start    # ğŸ¬ Launch dev server (http://localhost:5173) for live-coding bliss! ğŸ”¥
```

### ğŸ—ï¸ Architecture

#### Core Technologies

- **Frontend**: [React](https://react.dev/) with [TypeScript](https://www.typescriptlang.org/)
- **Styling**: [Tailwind CSS](https://tailwindcss.com/docs/) + [DaisyUI](https://daisyui.com/)
- **State Management**: React Context API
- **Routing**: [React Router](https://reactrouter.com/)
- **Storage**: IndexedDB via [Dexie.js](https://dexie.org/)
- **Build Tool**: [Vite](https://vite.dev/)

#### Key Components

1. **App Context**: Manages global configuration and settings
2. **Inference Context**: Handles API communication with inference providers
3. **Message Context**: Manages conversation state and message generation
4. **Storage Utils**: IndexedDB operations and localStorage management
5. **Inference API**: HTTP client for communicating with inference servers

## ğŸ“œ License - Freedom First!

llama.ui is proudly **MIT licensed** - go build amazing things! ğŸš€ See [LICENSE](LICENSE) for details.

---

<p align="center">
Made with â¤ï¸ and â˜• by humans who believe in private AI
</p>
