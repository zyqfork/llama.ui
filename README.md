# ğŸ¦™ llama.ui - Minimal Interface for Local AI Companion âœ¨

**Tired of complex AI setups?** ğŸ˜© `llama.ui` is an open-source desktop application that provides a beautiful âœ¨, user-friendly interface for interacting with large language models (LLMs) powered by `llama.cpp`. Designed for simplicity and privacy ğŸ”’, this project lets you chat with powerful quantized models on your local machine - no cloud required! ğŸš«â˜ï¸

## âš¡ TL;DR

This repository is a fork of [llama.cpp](https://github.com/ggml-org/llama.cpp) WebUI with:

- Fresh new styles ğŸ¨
- Extra functionality âš™ï¸
- Smoother experience âœ¨

![welcome-screen](public/screenshots/desktop.png)

## ğŸš€ Getting Started in 60 Seconds!

### ğŸ’» Standalone Mode (Zero Installation)

1. âœ¨ Open our [hosted UI instance](https://olegshulyakov.github.io/llama.ui/)
2. âš™ï¸ Click the gear icon â†’ General settings
3. ğŸŒ Set "Base URL" to your local llama.cpp server (e.g. `http://localhost:8080`)
4. ğŸ‰ Start chatting with your AI!

<details><summary><b>ğŸ”§ Need HTTPS magic for your local instance? Try this mitmproxy hack!</b></summary>
<p>

**Uh-oh!** Browsers block HTTP requests from HTTPS sites ğŸ˜¤. Since `llama.cpp` uses HTTP, we need a bridge ğŸŒ‰. Enter [mitmproxy](https://www.mitmproxy.org/) - our traffic wizard! ğŸ§™â€â™‚ï¸

**Local setup:**

```bash
mitmdump -p 8443 --mode reverse:http://localhost:8080/
```

**Docker quickstart:**

```bash
docker run -it -p 8443:8443 mitmproxy/mitmproxy mitmdump -p 8443 --mode reverse:http://localhost:8080/
```

**Pro-tip with Docker Compose:**

```yml
services:
  mitmproxy:
    container_name: mitmproxy
    image: mitmproxy/mitmproxy:latest
    ports:
      - '8443:8443' # ğŸ” Port magic happening here!
    command: mitmdump -p 8443 --mode reverse:http://localhost:8080/
    # ... (other config)
```

> âš ï¸ **Certificate Tango Time!**
>
> 1. Visit http://localhost:8443
> 2. Click "Trust this certificate" ğŸ¤
> 3. Restart ğŸ¦™ llama.ui page ğŸ”„
> 4. Profit! ğŸ’¸

**VoilÃ !** You've hacked the HTTPS barrier! ğŸ©âœ¨

</p>
</details>

### ğŸ–¥ï¸ Full Local Installation (Power User Edition)

1. ğŸ“¦ Grab the latest release from our [releases page](https://github.com/olegshulyakov/llama.ui/releases)
2. ğŸ—œï¸ Unpack the archive (feel that excitement! ğŸ¤©)
3. âš¡ Fire up your llama.cpp server:

**Linux/MacOS:**

```bash
./server --host 0.0.0.0 \
         --port 8080 \
         --path "/path/to/llama.ui" \
         -m models/llama-2-7b.Q4_0.gguf \
         --ctx-size 4096
```

**Windows:**

```bat
llama-server ^
             --host 0.0.0.0 ^
             --port 8080 ^
             --path "C:\path\to\llama.ui" ^
             -m models\mistral-7b.Q4_K_M.gguf ^
             --ctx-size 4096
```

4. ğŸŒ Visit http://localhost:8080 and meet your new AI buddy! ğŸ¤–â¤ï¸

## ğŸŒŸ Join Our Awesome Community!

**We're building something special together!** ğŸš€

- ğŸ¯ **PRs are welcome!** (Seriously, we high-five every contribution! âœ‹)
- ğŸ› **Bug squashing?** Yes please! ğŸ§¯
- ğŸ“š **Documentation heroes** needed! ğŸ¦¸
- âœ¨ **Make magic** with your commits! (Follow [Conventional Commits](https://www.conventionalcommits.org))

### ğŸ› ï¸ Developer Wonderland

**Prerequisites:**

- ğŸ’» macOS/Windows/Linux
- â¬¢ Node.js >= 22
- ğŸ¦™ Local [llama.cpp server](https://github.com/ggml-org/llama.cpp/tree/master/tools/server) humming along

**Build the future:**

```bash
npm ci       # ğŸ“¦ Grab dependencies
npm run build  # ğŸ”¨ Craft the magic
npm start    # ğŸ¬ Launch dev server (http://localhost:5173) for live-coding bliss! ğŸ”¥
```

## ğŸ“œ License - Freedom First!

llama.ui is proudly **MIT licensed** - go build amazing things! ğŸš€ See [LICENSE](LICENSE) for details.

---

<p align="center">
Made with â¤ï¸ and â˜• by humans who believe in private AI
</p>
